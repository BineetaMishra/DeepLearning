{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC_lyAgfkT3t"
   },
   "source": [
    "## ***CNN (Dog And Cat Classification)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vha2GU9CkeWK"
   },
   "source": [
    "# *Importing the libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.0-cp37-cp37m-win_amd64.whl (370.7 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 425, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\http\\client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\http\\client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 186, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 331, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 177, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 333, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 282, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 482, in prepare_linked_requirement\n",
      "    hashes=hashes,\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 287, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 159, in unpack_http_url\n",
      "    link, downloader, temp_dir.path, hashes\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 303, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\ui.py\", line 160, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 39, in response_chunks\n",
      "    decode_content=False,\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 529, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\ebineet\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 430, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ryEvABcLFrr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-49bfb1d602f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Importing the Libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Importing the Libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cz2KHKNbkyn4",
    "outputId": "b9b491d2-ddfd-4782-ca95-b2f2bf21b244"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the version of the tensorflow. It should be atleast 2.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48oLcW7yk7ha"
   },
   "source": [
    "# ***Part1: Data Preprocessing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7uYEv0mlDJe"
   },
   "source": [
    "# PreProcessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Qqg_sXvk1FB",
    "outputId": "537fd0c6-3b36-471f-bcac-2bcdc0aff86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8068 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the training set.\n",
    "#we will apply transformation on the training set because to avoid over fitting.\n",
    "#these transformation is called image augmentation. By applyingthese we get new set\n",
    "#ImageDataGenerator will create image transformation.train_datagen is instance of class\n",
    "#resclae=1./255 <-- this will apply feature scaling on each pixel by diving the value with. Just like Normalisatiom 225\n",
    "train_datagen= ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    '/content/drive/MyDrive/Dataset/dataset/training_set', #path\n",
    "    target_size=(64, 64), #thefinal size of image that will be fed to convolution ntwrk.\n",
    "    batch_size= 32, # how many images in each batch.\n",
    "    class_mode ='binary') #this can be binary (or) categorical depending on output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uc6O-V9pcuS"
   },
   "source": [
    "# PreProcessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtaZS2capdEh",
    "outputId": "a10e659b-80de-4090-8c1e-1d979ce30753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2020 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#no transformation is required to the test images\n",
    "#rescle their pixel, feature scale them\n",
    "#ONLY FEATURE SCLAING IS REQUIRED HERE.\n",
    "test_datagen= ImageDataGenerator(rescale=1./255)\n",
    "test_set =test_datagen.flow_from_directory(\n",
    "    '/content/drive/MyDrive/Dataset/dataset/test_set', #path\n",
    "    target_size=(64, 64), #predict method should be build on same format as on traing\n",
    "    batch_size= 32, # how many images in each batch.\n",
    "    class_mode ='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AW5GKHtrUDD"
   },
   "source": [
    "# ***Part2: Building the CNN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxTWo1plrhto"
   },
   "source": [
    "Initialing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QKcUOGV3rUVq"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvmy404Urkcy"
   },
   "source": [
    "Step1: Convolution\n",
    "\n",
    "2D convolution layer : This layer creates a convolution kernel that is convolved with the\n",
    "layer input to produce a tensor of outputs. If use_bias is true a bias vector is created, and added to otpt. And finally if Activationis not None it is applied to the outputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ok_yaYSBrnQ8"
   },
   "outputs": [],
   "source": [
    "#filters(no. of features)are 32 as per classic architecture.\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6KNp5nsrncI"
   },
   "source": [
    "Step2: Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6DpY_NkJrpFw"
   },
   "outputs": [],
   "source": [
    "#Max pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDfXDGg0rpYA"
   },
   "source": [
    "Adding a second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V9fKXn2yrtZ4"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH0EqHbKrto9"
   },
   "source": [
    "Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zFs4le73YaFw"
   },
   "outputs": [],
   "source": [
    "#flattening to 1d vector which will become input to FC layer. This class does not take anything as parameters\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_zuvbNPrvoe"
   },
   "source": [
    "Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jIYoI9LLYbT0"
   },
   "outputs": [],
   "source": [
    "#no of hidden neuron = units= 128 defaul\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stsVxZ2BrxQf"
   },
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-__b3Tr7rzI_"
   },
   "outputs": [],
   "source": [
    "#binary output --> sigmoid AF\n",
    "#multi class output --> Softmax\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcb2ScE_cTh8"
   },
   "source": [
    "# ***Training the CNN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZVT9OkScXnp"
   },
   "source": [
    "Compling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "l_CtbpXHcVg-"
   },
   "outputs": [],
   "source": [
    "#compile the cnn (optimiser, loss function and some metrics)\n",
    "#binary classification the loss function should be binary_crossentropy, and non binary is categorical_crossentropy\n",
    "#metrics goes in a [] because we can use many diff types of metrics and it should be entered as a list.\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18DIqNhWczar"
   },
   "source": [
    "Training the CNN on teh Training set and evaluating on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60LlU2upc3yZ",
    "outputId": "caa9d2e8-aa43-4ffa-f78e-b6345722657c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "253/253 [==============================] - 5652s 22s/step - loss: 0.6986 - accuracy: 0.5134 - val_loss: 0.6493 - val_accuracy: 0.6099\n",
      "Epoch 2/25\n",
      "253/253 [==============================] - 39s 156ms/step - loss: 0.6125 - accuracy: 0.6716 - val_loss: 0.5522 - val_accuracy: 0.7248\n",
      "Epoch 3/25\n",
      "253/253 [==============================] - 39s 154ms/step - loss: 0.5417 - accuracy: 0.7269 - val_loss: 0.5171 - val_accuracy: 0.7594\n",
      "Epoch 4/25\n",
      "253/253 [==============================] - 39s 153ms/step - loss: 0.4958 - accuracy: 0.7635 - val_loss: 0.5031 - val_accuracy: 0.7619\n",
      "Epoch 5/25\n",
      "253/253 [==============================] - 39s 155ms/step - loss: 0.4838 - accuracy: 0.7703 - val_loss: 0.5039 - val_accuracy: 0.7634\n",
      "Epoch 6/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.4525 - accuracy: 0.7920 - val_loss: 0.4826 - val_accuracy: 0.7782\n",
      "Epoch 7/25\n",
      "253/253 [==============================] - 39s 153ms/step - loss: 0.4293 - accuracy: 0.8007 - val_loss: 0.5265 - val_accuracy: 0.7554\n",
      "Epoch 8/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.3996 - accuracy: 0.8141 - val_loss: 0.4955 - val_accuracy: 0.7842\n",
      "Epoch 9/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.3546 - accuracy: 0.8457 - val_loss: 0.4784 - val_accuracy: 0.7762\n",
      "Epoch 10/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.3276 - accuracy: 0.8588 - val_loss: 0.5082 - val_accuracy: 0.7708\n",
      "Epoch 11/25\n",
      "253/253 [==============================] - 39s 153ms/step - loss: 0.2941 - accuracy: 0.8759 - val_loss: 0.5625 - val_accuracy: 0.7579\n",
      "Epoch 12/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.2550 - accuracy: 0.8969 - val_loss: 0.5152 - val_accuracy: 0.7916\n",
      "Epoch 13/25\n",
      "253/253 [==============================] - 39s 154ms/step - loss: 0.2126 - accuracy: 0.9136 - val_loss: 0.5693 - val_accuracy: 0.7866\n",
      "Epoch 14/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.2087 - accuracy: 0.9196 - val_loss: 0.6013 - val_accuracy: 0.7886\n",
      "Epoch 15/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.1655 - accuracy: 0.9364 - val_loss: 0.5893 - val_accuracy: 0.7936\n",
      "Epoch 16/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.1354 - accuracy: 0.9519 - val_loss: 0.6183 - val_accuracy: 0.7871\n",
      "Epoch 17/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.1206 - accuracy: 0.9563 - val_loss: 0.6742 - val_accuracy: 0.7832\n",
      "Epoch 18/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.0851 - accuracy: 0.9690 - val_loss: 0.7500 - val_accuracy: 0.7886\n",
      "Epoch 19/25\n",
      "253/253 [==============================] - 39s 152ms/step - loss: 0.0807 - accuracy: 0.9749 - val_loss: 0.7096 - val_accuracy: 0.7827\n",
      "Epoch 20/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.0658 - accuracy: 0.9805 - val_loss: 0.8603 - val_accuracy: 0.7881\n",
      "Epoch 21/25\n",
      "253/253 [==============================] - 39s 153ms/step - loss: 0.0537 - accuracy: 0.9844 - val_loss: 0.9540 - val_accuracy: 0.7955\n",
      "Epoch 22/25\n",
      "253/253 [==============================] - 38s 152ms/step - loss: 0.0444 - accuracy: 0.9868 - val_loss: 0.9490 - val_accuracy: 0.8005\n",
      "Epoch 23/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.0355 - accuracy: 0.9893 - val_loss: 1.0076 - val_accuracy: 0.7649\n",
      "Epoch 24/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.0430 - accuracy: 0.9862 - val_loss: 1.0681 - val_accuracy: 0.7772\n",
      "Epoch 25/25\n",
      "253/253 [==============================] - 38s 151ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 1.1206 - val_accuracy: 0.7743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fda1026ed68>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data= test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjCAbv2vdSDE"
   },
   "source": [
    "# ***Part4: Making a single Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9nG9S3mdVb8"
   },
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "\n",
    "import numpy as np # because of the array as input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#load the single image\n",
    "#image.load_img this function is used to load the image\n",
    "# image that will become input to the predict methof should have the same size as the training image\n",
    "test_image= image.load_img('/content/drive/MyDrive/Dataset/dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64))#--> first test img\n",
    "# in order to get accepted by the predict method some work should be done on the test image\n",
    "#convert the image format to a 2D array. Predict method accepts a 2D array as input\n",
    "test_image= image.img_to_array(test_image)\n",
    "#predict method should be called on exact same format as on training\n",
    "#batch_size should be craeeted for single image also. So that the cnn model can recognise the batch as extra dimension.\n",
    "test_image= np.expand_dims(test_image, axis=0)\n",
    "#call the predict method\n",
    "result= cnn.predict(test_image)\n",
    "#call the class indices attribute from training set.\n",
    "#hardcoding the above step\n",
    "print(training_set.class_indices)\n",
    "if result[0][0] ==1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Udemy_CNN_Task.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
