{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Restaurant_review_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "108eS_QjkBlp0QhKy1xmdrWunr9T2qK_t",
      "authorship_tag": "ABX9TyPH1Q9YHaSUEr5blHc8Nw7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BineetaMishra/DeepLearning/blob/main/Restaurant_review_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qSyUYPArFWg"
      },
      "source": [
        "# ***NATURAL LANGUAGE PROCESSING***\r\n",
        "\r\n",
        "This task will do the sentiment analysis of the Restaurant reviews. Bag of words Creation. will also happen here. The reviews will be predicted whether it is a negative r positive review.\r\n",
        "The datatset here is a tsv file. A csv file is a comma seperated file and a tsv file is tab sepersted Delimiter= \\t (tab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLd5wxJsXWe"
      },
      "source": [
        "# Importing the Librarires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ctIS17tWuJ"
      },
      "source": [
        "#importing the libraries\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab-qkeTwq-9H"
      },
      "source": [
        "# Importing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xz-wZAytXwB"
      },
      "source": [
        "#Importing the dataset. Here it is a tsv file\r\n",
        "#delimiter='\\t' used for tsv files\r\n",
        "#quoting =3, to ignoring the \"\"(double quotes) from the dateset.It is done to remove the sparsing error.\r\n",
        "#As it can lead to execution error.\r\n",
        "dataset= pd.read_csv('/content/drive/MyDrive/Dataset/Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8l8omOLsfFz"
      },
      "source": [
        "# Cleaning the Dataset.\r\n",
        "Deep cleaning of text. So far the text has punctuation, diff characters other than letters, Capital letters, lower case letters, numbers. Thus we need to simplify the text and this is called deep cleaning in order to ease the  learning process of the training process.\r\n",
        "The libraries used are:-\r\n",
        "1. **re** -->library. used to simplify the reviews\r\n",
        "2. **nltk** ->Classic library in NLP, which will allow us to download the emsemble of stop words.\r\n",
        "3. **StopWords**-->These are words we do not want to inclue in the reviews after cleaning the text as they are not prevelant to help classify the reviews as positive or negative. These words include-> Articles, Pronouns.\r\n",
        "4. **PorterStemmer** is a class which helps in apply stemming on reviews. Stemming consisites of taking only the root of the word which is enough to understand the word. example for \"loved\" it will take \"Love\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFHi2q5vqlU",
        "outputId": "bb63b90c-adeb-4966-c98d-6c5ccc07d92e"
      },
      "source": [
        "#Cleaning the datatset\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords') #--> downloads the stopwords\r\n",
        "from nltk.corpus import stopwords #--> imports the stopwords in our code.\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "corpus=[] #--> empty list to save teh new list\r\n",
        "# making a for loop to iterate through all the reviews of the dataset and cleaning the reviews..\r\n",
        "for i in range(0, 1000):\r\n",
        "  review= re.sub('[^a~zA~Z]', ' ',dataset['Review'][i] )#creting a cleaned review. Removing the punctuation from teh etxts.\r\n",
        "  review= review.lower() #transform all the Capital letters to Lower case\r\n",
        "  review= review.split() #prepare for stemming. Spilt diff elements in diff words. Then stemming can be applued to each word.\r\n",
        "  ps = PorterStemmer() #stemming. Simplifying each word by Root of teh word.\r\n",
        "  all_stopwords = stopwords.words('english')\r\n",
        "  all_stopwords.remove('not')\r\n",
        "  review= [ps.stem(word) for word in review if not word in set(all_stopwords)]\r\n",
        "  review= ' '.join(review)#joining the reviews again\r\n",
        "  corpus.append(review)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leJEJj7XshO8"
      },
      "source": [
        "# Creating the Bag of Words Model\r\n",
        "Create  a sparse matrix. All the words taken from all the reviews.\r\n",
        "Each cell will get 0 or 1. 0 if the word in the column is not in the review of the row. 1 if the word in the column is in the review of the row.\r\n",
        "The process of creating all the columns taken from each of the word taken from all the reviews is called ***Tokenization***. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DeRoeO_HmPj"
      },
      "source": [
        "# Proceeding with Tokenization to create the \r\n",
        "#Sparse matrix containing all the reviews in diff rows and all teh words from all the reviews in diff col.\r\n",
        "#1 if word is in review and 0 otherwise. We will do tokenization from sklearn  module called feature extraction \r\n",
        "#class--> CounterVectorizer\r\n",
        "#take most freq words in teh sparse matrix\r\n",
        "#fit the corpus to X, Take all the words from all the reviews in the corpus \r\n",
        "#and then use transform part to put all the words in the columns\r\n",
        "#toarray()---> matrix of features must be a 2D array. \r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "#cv= CountVectorizer(max_features=1500) use this only after we know how mnay words are there.\r\n",
        "cv= CountVectorizer()\r\n",
        "X= cv.fit_transform(corpus).toarray()\r\n",
        "Y= dataset.iloc[:, -1].values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8mpOvlUQfrq",
        "outputId": "0e493476-9c64-4645-edf2-33986b3cb7f1"
      },
      "source": [
        "X"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz-BMYlBMz2p",
        "outputId": "7acbc709-2a74-4f3b-9626-de895e00c74f"
      },
      "source": [
        "len(X[0]) # use 1500"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiIEnc65slz1"
      },
      "source": [
        "# Splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dmoPAwJuvc8"
      },
      "source": [
        "#Splitting the dataset into the training and test set.\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.25)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST7_4Cxosn3O"
      },
      "source": [
        "# Training the Dataset on the Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqTlZPr1NmrH",
        "outputId": "5bda5e0d-db28-4e9f-8554-bff0e103b654"
      },
      "source": [
        "## Training the Naive Bayes Model on the Training Set.\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "classifire = GaussianNB()\r\n",
        "classifire.fit(x_train,y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxwfKKlKswX0"
      },
      "source": [
        "# Predicting the Test set Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFR1RIi2Ns2m",
        "outputId": "e00df2c5-5291-4843-955e-a1564fea0186"
      },
      "source": [
        "#### Prediction the test set result\r\n",
        "\r\n",
        "y_pred = classifire.predict(x_test)\r\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFWH344wsw0C"
      },
      "source": [
        "# Making the confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14HrEFv1s3eR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55cd7a9-e54a-46be-c4d4-1b3840ca0df3"
      },
      "source": [
        "## Making Confusion Matrix\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "cm= confusion_matrix(y_test,y_pred)\r\n",
        "print(cm)\r\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[124   2]\n",
            " [111  13]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.548"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}